{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6c36e8-bf91-4d71-b26e-ce9ad4aca8d9",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "For our project we are going to use the wisdom of 3 most popular Reddit communities related to Data Science -\n",
    "* Machine Learning - [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)\n",
    "* Artificial Intelligence - [r/artificial](https://www.reddit.com/r/Artificial/)\n",
    "* Data Science - [r/DataScience](https://www.reddit.com/r/DataScience/)\n",
    "\n",
    "We will extract the required information using Reddit's official API - [PRAW](https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html) (The Python Reddit API Wrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8602a503-762b-4339-acd7-bf127bc4928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baed496-6805-4d4f-9265-8c450c89e1ed",
   "metadata": {},
   "source": [
    "The credentials required to access API can be procured from [reddit.com/prefs/apps](https://www.reddit.com/prefs/apps).\n",
    "The required credentials to access the API are provided though 'reddit_credentials.ini' to protect senstive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3b82a9-f3ec-4b06-a8a8-e284c894f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading configuration files for Reddit Credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('reddit_credentials.ini')\n",
    "\n",
    "# Storing credential info in local variables\n",
    "user_agent = config.get('credentials', 'user_agent')\n",
    "client_id = config.get('credentials', 'client_id')\n",
    "client_secret = config.get('credentials', 'client_secret')\n",
    "redirect_url = config.get('credentials', 'redirect_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f12986-0fd2-4f5a-b193-8a220828ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating read-only Reddit instance\n",
    "reddit = praw.Reddit(user_agent = user_agent,\n",
    "                    client_id = client_id,\n",
    "                    client_secret = client_secret,\n",
    "                    redirect_url = redirect_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87bd0c9-46c4-49c2-9981-c114bd9331cd",
   "metadata": {},
   "source": [
    "## Extracting Top Posts\n",
    "We will extract top 1000 post of all time from each sub-reddit to create our dataset along with some other useful information like Post URL (& ID), User posted, Post title, Flair, Number of Comments, Time Created, Upvote Ratio and Score.\n",
    "We will use this information further to analyse and infer useful insights from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eefec6b-326b-4e7a-8fb0-a97fcd76c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting top 1000 posts from each subreddit\n",
    "posts = reddit.subreddit('MachineLearning+artificial+datascience').top(time_filter = 'all', limit = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64308ea1-fd7c-4c15-a847-89aca16d1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creting DataFrame of the top posts along with other attributes for analysis\n",
    "\n",
    "posts_list = []\n",
    "\n",
    "for post in posts:\n",
    "    posts_list.append({\n",
    "        'post_id' : post.id,\n",
    "        'post_title' : post.title,\n",
    "        'subreddit' : post.subreddit,\n",
    "        'time_created' : post.created_utc,\n",
    "        'post_url' : post.url,\n",
    "        'flair_text' : post.link_flair_text,\n",
    "        'score' : post.score,\n",
    "        'comments' : post.num_comments,\n",
    "        'upvote_ratio' : post.upvote_ratio\n",
    "    })\n",
    "    \n",
    "posts_df = pd.DataFrame(posts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba0a57c4-ff32-4076-8ab8-8be80887144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting UTC Date format to Standard Date-Time format\n",
    "posts_df['date-time'] = posts_df['time_created'].apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "\n",
    "# Creating 'Year' column\n",
    "posts_df['year'] = posts_df['date-time'].dt.year\n",
    "\n",
    "# Dropping 'time_created' column\n",
    "posts_df.drop('time_created', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6287efdd-e8f1-4732-9b63-99651bd30195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our posts data in .csv format\n",
    "posts_df.to_csv(\"Top_Posts.csv\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "024525ac-5bc5-418e-afcb-90dec17976a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_url</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>date-time</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10mmm38</td>\n",
       "      <td>As a hiring manager - this, this right here</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://i.redd.it/fk95v2ghilea1.png</td>\n",
       "      <td>Career</td>\n",
       "      <td>2495</td>\n",
       "      <td>140</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2023-01-27 14:48:21</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>vljjur</td>\n",
       "      <td>How the AI be walking on the 17th generation</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://i.redd.it/abl4dixjf2891.gif</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>1243</td>\n",
       "      <td>19</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2022-06-27 01:24:27</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>10pb1y3</td>\n",
       "      <td>[P] I launched “CatchGPT”, a supervised model ...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>Project</td>\n",
       "      <td>492</td>\n",
       "      <td>211</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2023-01-30 19:09:14</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>qrjmge</td>\n",
       "      <td>Stop asking data scientist riddles in interviews!</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://i.redd.it/jjtjirwagyy71.jpg</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>2283</td>\n",
       "      <td>269</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2021-11-11 11:52:13</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>dv7mdc</td>\n",
       "      <td>\"If you torture the data long enough, it will ...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://i.redd.it/5rg06b0c38y31.png</td>\n",
       "      <td>Fun/Trivia</td>\n",
       "      <td>1151</td>\n",
       "      <td>34</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2019-11-12 09:20:56</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>5k35un</td>\n",
       "      <td>Deep Learning Enables You to Hide Screen when ...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>http://ahogrammer.com/2016/11/15/deep-learning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2016-12-24 14:06:16</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>gb08da</td>\n",
       "      <td>[P] I wrote an API to build neural networks in...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>Project</td>\n",
       "      <td>522</td>\n",
       "      <td>38</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2020-04-30 17:33:50</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>db8c4u</td>\n",
       "      <td>[N] UC Berkeley's CS 285: Deep Reinforcement L...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>News</td>\n",
       "      <td>362</td>\n",
       "      <td>31</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2019-09-30 08:05:51</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>jjbdfm</td>\n",
       "      <td>Probability practice problems</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>Job Search</td>\n",
       "      <td>247</td>\n",
       "      <td>31</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2020-10-27 22:21:07</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>amsdk2</td>\n",
       "      <td>Some Important Data Science Tools that aren’t ...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://towardsdatascience.com/some-important-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235</td>\n",
       "      <td>42</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2019-02-03 18:34:09</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                         post_title  \\\n",
       "41    10mmm38        As a hiring manager - this, this right here   \n",
       "180    vljjur       How the AI be walking on the 17th generation   \n",
       "896   10pb1y3  [P] I launched “CatchGPT”, a supervised model ...   \n",
       "49     qrjmge  Stop asking data scientist riddles in interviews!   \n",
       "202    dv7mdc  \"If you torture the data long enough, it will ...   \n",
       "2980   5k35un  Deep Learning Enables You to Hide Screen when ...   \n",
       "820    gb08da  [P] I wrote an API to build neural networks in...   \n",
       "1461   db8c4u  [N] UC Berkeley's CS 285: Deep Reinforcement L...   \n",
       "2065   jjbdfm                      Probability practice problems   \n",
       "2102   amsdk2  Some Important Data Science Tools that aren’t ...   \n",
       "\n",
       "            subreddit                                           post_url  \\\n",
       "41        datascience                https://i.redd.it/fk95v2ghilea1.png   \n",
       "180        artificial                https://i.redd.it/abl4dixjf2891.gif   \n",
       "896   MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "49        datascience                https://i.redd.it/jjtjirwagyy71.jpg   \n",
       "202       datascience                https://i.redd.it/5rg06b0c38y31.png   \n",
       "2980       artificial  http://ahogrammer.com/2016/11/15/deep-learning...   \n",
       "820   MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "1461  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "2065      datascience  https://www.reddit.com/r/datascience/comments/...   \n",
       "2102      datascience  https://towardsdatascience.com/some-important-...   \n",
       "\n",
       "      flair_text  score  comments  upvote_ratio            date-time  year  \n",
       "41        Career   2495       140          0.96  2023-01-27 14:48:21  2023  \n",
       "180   Discussion   1243        19          0.98  2022-06-27 01:24:27  2022  \n",
       "896      Project    492       211          0.75  2023-01-30 19:09:14  2023  \n",
       "49    Discussion   2283       269          0.94  2021-11-11 11:52:13  2021  \n",
       "202   Fun/Trivia   1151        34          0.98  2019-11-12 09:20:56  2019  \n",
       "2980         NaN     82         2          0.95  2016-12-24 14:06:16  2016  \n",
       "820      Project    522        38          0.98  2020-04-30 17:33:50  2020  \n",
       "1461        News    362        31          0.97  2019-09-30 08:05:51  2019  \n",
       "2065  Job Search    247        31          0.98  2020-10-27 22:21:07  2020  \n",
       "2102         NaN    235        42          0.96  2019-02-03 18:34:09  2019  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the content of saved Post Data\n",
    "posts_df = pd.read_csv('Top_Posts.csv')\n",
    "posts_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973351e-f057-447d-bcd5-83a0887b95a8",
   "metadata": {},
   "source": [
    "## Extracting Comments\n",
    "Using 'post_id' of top posts we will further extract all comments. We will create a different dataset containing 'post_id' and 'comment' to create our textual dataset for training our large NLP model (GPT-3.5-turbo). We will also utilize this data to analyse the sentiment aroud different topics and recognizing emotions of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46af94-f400-4a90-8651-5aead766b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame of all the comments available in the Top Posts\n",
    "\n",
    "comments_list = []\n",
    "\n",
    "for post_id in posts_df['post_id']:\n",
    "    submission = reddit.submission(post_id)\n",
    "    submission.comments.replace_more(limit = None)\n",
    "    \n",
    "    for comment in submission.comments.list():\n",
    "        comments_list.append({\n",
    "            'post_id' : post_id,\n",
    "            'comment' : comment.body\n",
    "        })\n",
    "        \n",
    "comments_df = pd.DataFrame(comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847cf49-6c66-4288-b8c1-f095c9766501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our comments data in .csv format\n",
    "comments_df.to_csv('Top_Posts_Comments.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "422ebdfc-0d34-4ebd-b0e2-ae163fe95479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61970</th>\n",
       "      <td>11w03sy</td>\n",
       "      <td>!remindme one week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146623</th>\n",
       "      <td>2lmo0l</td>\n",
       "      <td>Hello Dr Hinton, Im doing a case study in my c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96224</th>\n",
       "      <td>r76igz</td>\n",
       "      <td>Transformers robots in disguise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62757</th>\n",
       "      <td>ulvdgm</td>\n",
       "      <td>Love your work, scared of your name, uncertain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185205</th>\n",
       "      <td>kf2j1l</td>\n",
       "      <td>What does dagster bring to airflow that airflo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200955</th>\n",
       "      <td>riup34</td>\n",
       "      <td>Great comment! I'm a hybrid data engineer/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>hohvgq</td>\n",
       "      <td>Average DS guy from a business undergrad. Don’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169021</th>\n",
       "      <td>b3zlha</td>\n",
       "      <td>gpt-2 finish this\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135045</th>\n",
       "      <td>65ukie</td>\n",
       "      <td>You obvoiusly need to search better in the lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214825</th>\n",
       "      <td>bl6gbm</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                            comment\n",
       "61970   11w03sy                                 !remindme one week\n",
       "146623   2lmo0l  Hello Dr Hinton, Im doing a case study in my c...\n",
       "96224    r76igz                    Transformers robots in disguise\n",
       "62757    ulvdgm  Love your work, scared of your name, uncertain...\n",
       "185205   kf2j1l  What does dagster bring to airflow that airflo...\n",
       "200955   riup34  Great comment! I'm a hybrid data engineer/data...\n",
       "2316     hohvgq  Average DS guy from a business undergrad. Don’...\n",
       "169021   b3zlha                              gpt-2 finish this\\n\\n\n",
       "135045   65ukie  You obvoiusly need to search better in the lat...\n",
       "214825   bl6gbm                                          [deleted]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the content of our Comments Data\n",
    "comments_df = pd.read_csv('Top_Posts_Comments.csv')\n",
    "comments_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4237b9-af54-4788-a8ee-3d705831b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Posts Data - (2987, 9)\n",
      "Shape of Comments Data - (223174, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Posts Data - {}\".format(posts_df.shape))\n",
    "print(\"Shape of Comments Data - {}\".format(comments_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294eeaa-ac03-4d96-8fa2-49b4f6ff2d1f",
   "metadata": {},
   "source": [
    "We have successfully extracted ~223K comments from top 1000 posts from popular sub-reddits related to Data Science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit_project:Python",
   "language": "python",
   "name": "conda-env-reddit_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
