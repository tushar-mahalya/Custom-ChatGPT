{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a1faff-8be8-4318-96e9-1d5d25e8174c",
   "metadata": {},
   "source": [
    "![reddit banner](https://cdn.dribbble.com/users/1761084/screenshots/3587716/reddit.gtif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8602a503-762b-4339-acd7-bf127bc4928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3b82a9-f3ec-4b06-a8a8-e284c894f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading configuration files for Reddit Credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('reddit_credentials.ini')\n",
    "\n",
    "# Storing credential info in local variables\n",
    "user_agent = config.get('credentials', 'user_agent')\n",
    "client_id = config.get('credentials', 'client_id')\n",
    "client_secret = config.get('credentials', 'client_secret')\n",
    "redirect_url = config.get('credentials', 'redirect_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f12986-0fd2-4f5a-b193-8a220828ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating read-only Reddit instance\n",
    "reddit = praw.Reddit(user_agent = user_agent,\n",
    "                    client_id = client_id,\n",
    "                    client_secret = client_secret,\n",
    "                    redirect_url = redirect_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87bd0c9-46c4-49c2-9981-c114bd9331cd",
   "metadata": {},
   "source": [
    "## Extracting Comments\n",
    "For our project we are going to use top 3 most popular Reddit communities -\n",
    "* Machine Learning - [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)\n",
    "* Artificial Intelligence - [r/artificial](https://www.reddit.com/r/Artificial/)\n",
    "* Data Science - [r/DataScience](https://www.reddit.com/r/DataScience/)\n",
    "\n",
    "We will extract top 1000 post of all time from each sub-reddit to create our dataset along with some other useful information like Post URL (& ID), User posted, Post title, Flair, Number of Comments, Time Created, Upvote Ratio and Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eefec6b-326b-4e7a-8fb0-a97fcd76c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting top 1000 posts from each subreddit\n",
    "posts = reddit.subreddit('MachineLearning+artificial+datascience').top(time_filter = 'all', limit = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64308ea1-fd7c-4c15-a847-89aca16d1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creting DataFrame of the top posts along with other attributes for analysis\n",
    "\n",
    "posts_list = []\n",
    "\n",
    "for post in posts:\n",
    "    posts_list.append({\n",
    "        'post_id' : post.id,\n",
    "        'post_title' : post.title,\n",
    "        'subreddit' : post.subreddit,\n",
    "        'time_created' : post.created_utc,\n",
    "        'post_url' : post.url,\n",
    "        'flair_text' : post.link_flair_text,\n",
    "        'score' : post.score,\n",
    "        'comments' : post.num_comments,\n",
    "        'upvote_ratio' : post.upvote_ratio\n",
    "    })\n",
    "    \n",
    "posts_df = pd.DataFrame(posts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba0a57c4-ff32-4076-8ab8-8be80887144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting UTC Date format to Standard Date-Time format\n",
    "posts_df['date-time'] = posts_df['time_created'].apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "\n",
    "# Creating 'Year' column\n",
    "posts_df['year'] = posts_df['date-time'].dt.year\n",
    "\n",
    "# Dropping 'time_created' column\n",
    "posts_df.drop('time_created', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c2fc3e7-01f3-477c-b3bb-33a3acae40bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_url</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>date-time</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gh1dj9</td>\n",
       "      <td>[Project] From books to presentations in 10s w...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://v.redd.it/v492uoheuxx41</td>\n",
       "      <td>Project</td>\n",
       "      <td>7798</td>\n",
       "      <td>186</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2020-05-10 13:19:54</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kuc6tz</td>\n",
       "      <td>[D] A Demo from 1993 of 32-year-old Yann LeCun...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://v.redd.it/25nxi9ojfha61</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>5851</td>\n",
       "      <td>133</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2021-01-10 10:30:36</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g7nfvb</td>\n",
       "      <td>[R] First Order Motion Model applied to animat...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://v.redd.it/rlmmjm1q5wu41</td>\n",
       "      <td>Research</td>\n",
       "      <td>4761</td>\n",
       "      <td>111</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2020-04-25 04:27:23</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lui92h</td>\n",
       "      <td>[N] AI can turn old photos into moving Images ...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://v.redd.it/ikd5gjlbi8k61</td>\n",
       "      <td>News</td>\n",
       "      <td>4688</td>\n",
       "      <td>230</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2021-02-28 15:12:28</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ohxnts</td>\n",
       "      <td>[D] This AI reveals how much time politicians ...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://i.redd.it/34sgziebfia71.jpg</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>4568</td>\n",
       "      <td>228</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2021-07-11 04:18:59</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>slx33m</td>\n",
       "      <td>We live in beautiful times where you can learn...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://github.com/louisfb01/start-machine-lea...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>84</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2022-02-06 13:50:02</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>k9otbj</td>\n",
       "      <td>Yann LeCun’s Deep Learning Course Free From NYU</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://www.i-programmer.info/news/99-professi...</td>\n",
       "      <td>News</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2020-12-09 09:22:52</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>k2orib</td>\n",
       "      <td>You Can Now Learn for FREE: 9 Courses by Googl...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://laconicml.com/free-artificial-intellig...</td>\n",
       "      <td>Self Promotion</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2020-11-28 14:43:43</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>ex9w4w</td>\n",
       "      <td>Chatbot trained on \"public domain social media...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://ai.googleblog.com/2020/01/towards-conv...</td>\n",
       "      <td>news</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2020-02-01 17:55:23</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>efk5n3</td>\n",
       "      <td>Tesla's Neural Net can now identify red and gr...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://www.teslarati.com/tesla-holiday-update...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2019-12-25 18:50:50</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2987 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     post_id                                         post_title  \\\n",
       "0     gh1dj9  [Project] From books to presentations in 10s w...   \n",
       "1     kuc6tz  [D] A Demo from 1993 of 32-year-old Yann LeCun...   \n",
       "2     g7nfvb  [R] First Order Motion Model applied to animat...   \n",
       "3     lui92h  [N] AI can turn old photos into moving Images ...   \n",
       "4     ohxnts  [D] This AI reveals how much time politicians ...   \n",
       "...      ...                                                ...   \n",
       "2982  slx33m  We live in beautiful times where you can learn...   \n",
       "2983  k9otbj    Yann LeCun’s Deep Learning Course Free From NYU   \n",
       "2984  k2orib  You Can Now Learn for FREE: 9 Courses by Googl...   \n",
       "2985  ex9w4w  Chatbot trained on \"public domain social media...   \n",
       "2986  efk5n3  Tesla's Neural Net can now identify red and gr...   \n",
       "\n",
       "            subreddit                                           post_url  \\\n",
       "0     MachineLearning                    https://v.redd.it/v492uoheuxx41   \n",
       "1     MachineLearning                    https://v.redd.it/25nxi9ojfha61   \n",
       "2     MachineLearning                    https://v.redd.it/rlmmjm1q5wu41   \n",
       "3     MachineLearning                    https://v.redd.it/ikd5gjlbi8k61   \n",
       "4     MachineLearning                https://i.redd.it/34sgziebfia71.jpg   \n",
       "...               ...                                                ...   \n",
       "2982       artificial  https://github.com/louisfb01/start-machine-lea...   \n",
       "2983       artificial  https://www.i-programmer.info/news/99-professi...   \n",
       "2984       artificial  https://laconicml.com/free-artificial-intellig...   \n",
       "2985       artificial  https://ai.googleblog.com/2020/01/towards-conv...   \n",
       "2986       artificial  https://www.teslarati.com/tesla-holiday-update...   \n",
       "\n",
       "          flair_text  score  comments  upvote_ratio           date-time  year  \n",
       "0            Project   7798       186          0.99 2020-05-10 13:19:54  2020  \n",
       "1         Discussion   5851       133          0.98 2021-01-10 10:30:36  2021  \n",
       "2           Research   4761       111          0.97 2020-04-25 04:27:23  2020  \n",
       "3               News   4688       230          0.97 2021-02-28 15:12:28  2021  \n",
       "4         Discussion   4568       228          0.96 2021-07-11 04:18:59  2021  \n",
       "...              ...    ...       ...           ...                 ...   ...  \n",
       "2982      Discussion     84         6          0.90 2022-02-06 13:50:02  2022  \n",
       "2983            News     78         1          0.97 2020-12-09 09:22:52  2020  \n",
       "2984  Self Promotion     80         2          0.95 2020-11-28 14:43:43  2020  \n",
       "2985            news     80        10          0.97 2020-02-01 17:55:23  2020  \n",
       "2986             NaN     80        10          0.89 2019-12-25 18:50:50  2019  \n",
       "\n",
       "[2987 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6287efdd-e8f1-4732-9b63-99651bd30195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our posts data in .csv format\n",
    "posts_df.to_csv(\"Top_Posts.csv\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024525ac-5bc5-418e-afcb-90dec17976a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>time_created</th>\n",
       "      <th>post_url</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>8h1saq</td>\n",
       "      <td>Facebook is using billions of Instagram images...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1.525461e+09</td>\n",
       "      <td>https://www.theverge.com/2018/5/2/17311808/fac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113</td>\n",
       "      <td>24</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2851</th>\n",
       "      <td>10cfef6</td>\n",
       "      <td>Inpainting with the Visuali editor (beta)</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1.673774e+09</td>\n",
       "      <td>https://v.redd.it/4ncbg5mgv3ca1</td>\n",
       "      <td>Research</td>\n",
       "      <td>88</td>\n",
       "      <td>6</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>117bptb</td>\n",
       "      <td>PyGWalker: Turn your Pandas Dataframe into a T...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>1.676910e+09</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>Projects</td>\n",
       "      <td>472</td>\n",
       "      <td>47</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>5shha6</td>\n",
       "      <td>Fear at the top: The CEO of Google DeepMind is...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1.486421e+09</td>\n",
       "      <td>http://www.businessinsider.com/google-deepmind...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>34</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>btgj82</td>\n",
       "      <td>AI Trained on 100 Million Opinions Can Predict...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1.558924e+09</td>\n",
       "      <td>https://blog.photofeeler.com/photofeeler-d3/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132</td>\n",
       "      <td>27</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>zvhy5w</td>\n",
       "      <td>PaLM vs. ChatGPT: Who Will Win the AI Race?</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1.672040e+09</td>\n",
       "      <td>https://medium.com/inkwater-atlas/palm-vs-chat...</td>\n",
       "      <td>Self Promotion</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>i2bvrr</td>\n",
       "      <td>[P] Open RL Benchmark @ 0.3.0 (benchmark.clean...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.596374e+09</td>\n",
       "      <td>https://v.redd.it/80lthq5cale51</td>\n",
       "      <td>Project</td>\n",
       "      <td>350</td>\n",
       "      <td>14</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>hgttkm</td>\n",
       "      <td>This AI translates code from a programming lan...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1.593266e+09</td>\n",
       "      <td>https://youtu.be/u6kM2lkrGQk</td>\n",
       "      <td>News</td>\n",
       "      <td>113</td>\n",
       "      <td>12</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>j0m182</td>\n",
       "      <td>Jump Rope + AI. Keeping both on point! Made th...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1.601187e+09</td>\n",
       "      <td>https://v.redd.it/5fr03wigsmp51</td>\n",
       "      <td>My project</td>\n",
       "      <td>214</td>\n",
       "      <td>11</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>8d3zuy</td>\n",
       "      <td>Scientists develop artificial intelligence sys...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1.524039e+09</td>\n",
       "      <td>https://www.financialexpress.com/lifestyle/sci...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                         post_title  \\\n",
       "2562   8h1saq  Facebook is using billions of Instagram images...   \n",
       "2851  10cfef6          Inpainting with the Visuali editor (beta)   \n",
       "960   117bptb  PyGWalker: Turn your Pandas Dataframe into a T...   \n",
       "2701   5shha6  Fear at the top: The CEO of Google DeepMind is...   \n",
       "2432   btgj82  AI Trained on 100 Million Opinions Can Predict...   \n",
       "2916   zvhy5w        PaLM vs. ChatGPT: Who Will Win the AI Race?   \n",
       "1545   i2bvrr  [P] Open RL Benchmark @ 0.3.0 (benchmark.clean...   \n",
       "2532   hgttkm  This AI translates code from a programming lan...   \n",
       "2155   j0m182  Jump Rope + AI. Keeping both on point! Made th...   \n",
       "2787   8d3zuy  Scientists develop artificial intelligence sys...   \n",
       "\n",
       "            subreddit  time_created  \\\n",
       "2562       artificial  1.525461e+09   \n",
       "2851       artificial  1.673774e+09   \n",
       "960       datascience  1.676910e+09   \n",
       "2701       artificial  1.486421e+09   \n",
       "2432       artificial  1.558924e+09   \n",
       "2916       artificial  1.672040e+09   \n",
       "1545  MachineLearning  1.596374e+09   \n",
       "2532       artificial  1.593266e+09   \n",
       "2155       artificial  1.601187e+09   \n",
       "2787       artificial  1.524039e+09   \n",
       "\n",
       "                                               post_url      flair_text  \\\n",
       "2562  https://www.theverge.com/2018/5/2/17311808/fac...             NaN   \n",
       "2851                    https://v.redd.it/4ncbg5mgv3ca1        Research   \n",
       "960   https://www.reddit.com/r/datascience/comments/...        Projects   \n",
       "2701  http://www.businessinsider.com/google-deepmind...             NaN   \n",
       "2432       https://blog.photofeeler.com/photofeeler-d3/             NaN   \n",
       "2916  https://medium.com/inkwater-atlas/palm-vs-chat...  Self Promotion   \n",
       "1545                    https://v.redd.it/80lthq5cale51         Project   \n",
       "2532                       https://youtu.be/u6kM2lkrGQk            News   \n",
       "2155                    https://v.redd.it/5fr03wigsmp51      My project   \n",
       "2787  https://www.financialexpress.com/lifestyle/sci...             NaN   \n",
       "\n",
       "      score  comments  upvote_ratio  \n",
       "2562    113        24          0.94  \n",
       "2851     88         6          0.96  \n",
       "960     472        47          0.99  \n",
       "2701     98        34          0.90  \n",
       "2432    132        27          0.99  \n",
       "2916     87         3          0.93  \n",
       "1545    350        14          0.96  \n",
       "2532    113        12          0.97  \n",
       "2155    214        11          0.95  \n",
       "2787     91         2          0.97  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the content of saved Post Data\n",
    "posts_df = pd.read_csv('Top_Posts.csv')\n",
    "posts_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6c1b6-4617-4c92-99c8-7fe9dc025f1d",
   "metadata": {},
   "source": [
    "We will use the 'post_id' to further extract the comments from the Top Posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46af94-f400-4a90-8651-5aead766b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame of all the comments available in the Top Posts\n",
    "\n",
    "comments_list = []\n",
    "\n",
    "for post_id in posts_df['post_id']:\n",
    "    submission = reddit.submission(post_id)\n",
    "    submission.comments.replace_more(limit = None)\n",
    "    \n",
    "    for comment in submission.comments.list():\n",
    "        comments_list.append({\n",
    "            'post_id' : post_id,\n",
    "            'comment' : comment.body\n",
    "        })\n",
    "        \n",
    "comments_df = pd.DataFrame(comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847cf49-6c66-4288-b8c1-f095c9766501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our comments data in .csv format\n",
    "comments_df.to_csv('Top_Posts_Comments.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "422ebdfc-0d34-4ebd-b0e2-ae163fe95479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61970</th>\n",
       "      <td>11w03sy</td>\n",
       "      <td>!remindme one week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146623</th>\n",
       "      <td>2lmo0l</td>\n",
       "      <td>Hello Dr Hinton, Im doing a case study in my c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96224</th>\n",
       "      <td>r76igz</td>\n",
       "      <td>Transformers robots in disguise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62757</th>\n",
       "      <td>ulvdgm</td>\n",
       "      <td>Love your work, scared of your name, uncertain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185205</th>\n",
       "      <td>kf2j1l</td>\n",
       "      <td>What does dagster bring to airflow that airflo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200955</th>\n",
       "      <td>riup34</td>\n",
       "      <td>Great comment! I'm a hybrid data engineer/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>hohvgq</td>\n",
       "      <td>Average DS guy from a business undergrad. Don’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169021</th>\n",
       "      <td>b3zlha</td>\n",
       "      <td>gpt-2 finish this\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135045</th>\n",
       "      <td>65ukie</td>\n",
       "      <td>You obvoiusly need to search better in the lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214825</th>\n",
       "      <td>bl6gbm</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                            comment\n",
       "61970   11w03sy                                 !remindme one week\n",
       "146623   2lmo0l  Hello Dr Hinton, Im doing a case study in my c...\n",
       "96224    r76igz                    Transformers robots in disguise\n",
       "62757    ulvdgm  Love your work, scared of your name, uncertain...\n",
       "185205   kf2j1l  What does dagster bring to airflow that airflo...\n",
       "200955   riup34  Great comment! I'm a hybrid data engineer/data...\n",
       "2316     hohvgq  Average DS guy from a business undergrad. Don’...\n",
       "169021   b3zlha                              gpt-2 finish this\\n\\n\n",
       "135045   65ukie  You obvoiusly need to search better in the lat...\n",
       "214825   bl6gbm                                          [deleted]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the content of our Comments Data\n",
    "comments_df = pd.read_csv('Top_Posts_Comments.csv')\n",
    "comments_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4237b9-af54-4788-a8ee-3d705831b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Posts Data - (2987, 9)\n",
      "Shape of Comments Data - (223174, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Posts Data - {}\".format(posts_df.shape))\n",
    "print(\"Shape of Comments Data - {}\".format(comments_df.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit_project:Python",
   "language": "python",
   "name": "conda-env-reddit_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
