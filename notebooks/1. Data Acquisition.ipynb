{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622cb181-f0b8-4b54-857c-2188a8565f6c",
   "metadata": {},
   "source": [
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/tushar-mahalya/Custom-ChatGPT/blob/master/data/reddit_data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c36e8-bf91-4d71-b26e-ce9ad4aca8d9",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "For our project we are going to use the wisdom of 3 most popular Reddit communities related to Data Science -\n",
    "* Machine Learning - [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)\n",
    "* Artificial Intelligence - [r/artificial](https://www.reddit.com/r/Artificial/)\n",
    "* Data Science - [r/DataScience](https://www.reddit.com/r/DataScience/)\n",
    "\n",
    "We will extract the required information using Reddit's official API - [PRAW](https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html) (The Python Reddit API Wrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8602a503-762b-4339-acd7-bf127bc4928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important libraries\n",
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baed496-6805-4d4f-9265-8c450c89e1ed",
   "metadata": {},
   "source": [
    "The credentials required to access API can be procured from [reddit.com/prefs/apps](https://www.reddit.com/prefs/apps).\n",
    "\n",
    "For this project, I have saved my credentials in 'credentials.ini' file to protect my sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af3b82a9-f3ec-4b06-a8a8-e284c894f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading configuration files for Reddit Credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/home/studio-lab-user/sagemaker-studiolab-notebooks/Custom ChatGPT/credentials.ini')\n",
    "\n",
    "# Storing Reddit Credential info in local variables\n",
    "user_agent = config.get('Reddit', 'user_agent')\n",
    "client_id = config.get('Reddit', 'client_id')\n",
    "client_secret = config.get('Reddit', 'client_secret')\n",
    "redirect_url = config.get('Reddit', 'redirect_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07f12986-0fd2-4f5a-b193-8a220828ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating read-only Reddit instance\n",
    "reddit = praw.Reddit(user_agent = user_agent,\n",
    "                    client_id = client_id,\n",
    "                    client_secret = client_secret,\n",
    "                    redirect_url = redirect_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87bd0c9-46c4-49c2-9981-c114bd9331cd",
   "metadata": {},
   "source": [
    "## Extracting Top Posts\n",
    "We will extract top 1000 post of all time from each sub-reddit to create our dataset along with some other useful information like Post URL (& ID), User posted, Post title, Flair, Number of Comments, Time Created, Upvote Ratio and Score.\n",
    "We will use this information further to analyse and infer useful insights from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eefec6b-326b-4e7a-8fb0-a97fcd76c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting top 1000 posts from each subreddit\n",
    "posts = reddit.subreddit('MachineLearning+artificial+datascience').top(time_filter = 'all', limit = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64308ea1-fd7c-4c15-a847-89aca16d1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame of the top posts along with other attributes for analysis\n",
    "\n",
    "posts_list = []\n",
    "\n",
    "for post in posts:\n",
    "    posts_list.append({\n",
    "        'post_id' : post.id,\n",
    "        'post_title' : post.title,\n",
    "        'subreddit' : post.subreddit,\n",
    "        'time_created' : post.created_utc,\n",
    "        'post_url' : post.url,\n",
    "        'flair_text' : post.link_flair_text,\n",
    "        'score' : post.score,\n",
    "        'comments' : post.num_comments,\n",
    "        'upvote_ratio' : post.upvote_ratio\n",
    "    })\n",
    "    \n",
    "posts_df = pd.DataFrame(posts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba0a57c4-ff32-4076-8ab8-8be80887144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting UTC Date format to Standard Date-Time format\n",
    "posts_df['date-time'] = posts_df['time_created'].apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "\n",
    "# Creating 'Year' column\n",
    "posts_df['year'] = posts_df['date-time'].dt.year\n",
    "\n",
    "# Dropping 'time_created' column\n",
    "posts_df.drop('time_created', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6287efdd-e8f1-4732-9b63-99651bd30195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our posts data in .csv format\n",
    "posts_df.to_csv(\"/home/studio-lab-user/sagemaker-studiolab-notebooks/Custom ChatGPT/data/Top_Posts.csv\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024525ac-5bc5-418e-afcb-90dec17976a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_url</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>date-time</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>11h3p2x</td>\n",
       "      <td>[D] Facebooks LLaMA leaks via torrent file in PR</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>497</td>\n",
       "      <td>164</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2023-03-03 15:37:03</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>i54se2</td>\n",
       "      <td>Human-like robot hand mimicking demo</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://www.youtube.com/watch?v=ujZRmFbrCmQ&amp;fe...</td>\n",
       "      <td>My project</td>\n",
       "      <td>137</td>\n",
       "      <td>10</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2020-08-07 01:35:29</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>6xvnwo</td>\n",
       "      <td>[D] My Neural Network isn't working! What shou...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>http://theorangeduck.com/page/neural-network-n...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>442</td>\n",
       "      <td>62</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2017-09-03 20:44:08</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>83mkrz</td>\n",
       "      <td>The Brain Is The Most Important Organ You Have</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://i.redd.it/8n8r6ze9u4l01.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>385</td>\n",
       "      <td>14</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2018-03-11 13:02:03</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>gx95jt</td>\n",
       "      <td>Researchers make algorithm to generate frontal...</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://i.imgur.com/ShgRhum.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130</td>\n",
       "      <td>66</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2020-06-05 17:39:40</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                         post_title  \\\n",
       "872   11h3p2x   [D] Facebooks LLaMA leaks via torrent file in PR   \n",
       "2389   i54se2               Human-like robot hand mimicking demo   \n",
       "1073   6xvnwo  [D] My Neural Network isn't working! What shou...   \n",
       "1370   83mkrz     The Brain Is The Most Important Organ You Have   \n",
       "2450   gx95jt  Researchers make algorithm to generate frontal...   \n",
       "\n",
       "            subreddit                                           post_url  \\\n",
       "872   MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "2389       artificial  https://www.youtube.com/watch?v=ujZRmFbrCmQ&fe...   \n",
       "1073  MachineLearning  http://theorangeduck.com/page/neural-network-n...   \n",
       "1370       artificial                https://i.redd.it/8n8r6ze9u4l01.jpg   \n",
       "2450       artificial                    https://i.imgur.com/ShgRhum.jpg   \n",
       "\n",
       "      flair_text  score  comments  upvote_ratio            date-time  year  \n",
       "872   Discussion    497       164          0.98  2023-03-03 15:37:03  2023  \n",
       "2389  My project    137        10          0.99  2020-08-07 01:35:29  2020  \n",
       "1073  Discussion    442        62          0.95  2017-09-03 20:44:08  2017  \n",
       "1370         NaN    385        14          0.92  2018-03-11 13:02:03  2018  \n",
       "2450         NaN    130        66          0.78  2020-06-05 17:39:40  2020  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the content of saved Post Data\n",
    "posts_df = pd.read_csv('/home/studio-lab-user/sagemaker-studiolab-notebooks/Custom ChatGPT/data/Top_Posts.csv')\n",
    "posts_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973351e-f057-447d-bcd5-83a0887b95a8",
   "metadata": {},
   "source": [
    "## Extracting Comments\n",
    "Using 'post_id' of top posts we will further extract all comments. We will create a different dataset containing 'post_id' and 'comment' to create our textual dataset for training our large NLP model (GPT-3.5-turbo). We will also utilize this data to analyse the sentiment around different topics and recognizing emotions of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46af94-f400-4a90-8651-5aead766b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame of all the comments available in the Top Posts\n",
    "\n",
    "comments_list = []\n",
    "\n",
    "for post_id in posts_df['post_id']:\n",
    "    submission = reddit.submission(post_id)\n",
    "    submission.comments.replace_more(limit = None)\n",
    "    \n",
    "    for comment in submission.comments.list():\n",
    "        comments_list.append({\n",
    "            'post_id' : post_id,\n",
    "            'comment' : comment.body\n",
    "        })\n",
    "        \n",
    "comments_df = pd.DataFrame(comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847cf49-6c66-4288-b8c1-f095c9766501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our comments data in .csv format\n",
    "comments_df.to_csv('/home/studio-lab-user/sagemaker-studiolab-notebooks/Custom ChatGPT/data/Top_Posts_Comments.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "422ebdfc-0d34-4ebd-b0e2-ae163fe95479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123678</th>\n",
       "      <td>upl33c</td>\n",
       "      <td>I am very surprised someone like Nando would s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217492</th>\n",
       "      <td>whg1zi</td>\n",
       "      <td>I will have to check them out. Midjourney crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141273</th>\n",
       "      <td>404r9m</td>\n",
       "      <td>Deep learning has grown so fast over the last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114747</th>\n",
       "      <td>jc1fp2</td>\n",
       "      <td>Sorry I don't have time to perform test, you c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38746</th>\n",
       "      <td>heiyqq</td>\n",
       "      <td>Please read the article linked in OP.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13267</th>\n",
       "      <td>umse6v</td>\n",
       "      <td>If you PM I can send it there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42828</th>\n",
       "      <td>p29bae</td>\n",
       "      <td>lol you had me at \"unpaid\"..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131394</th>\n",
       "      <td>skjjvm</td>\n",
       "      <td>Do you think fine-tuning transformers in class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25050</th>\n",
       "      <td>o468ms</td>\n",
       "      <td>This is an excellent resource for reviewing ML...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194142</th>\n",
       "      <td>rvya1w</td>\n",
       "      <td>You give me the numbers and I'll tell you the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id                                            comment\n",
       "123678  upl33c  I am very surprised someone like Nando would s...\n",
       "217492  whg1zi  I will have to check them out. Midjourney crea...\n",
       "141273  404r9m  Deep learning has grown so fast over the last ...\n",
       "114747  jc1fp2  Sorry I don't have time to perform test, you c...\n",
       "38746   heiyqq              Please read the article linked in OP.\n",
       "13267   umse6v                      If you PM I can send it there\n",
       "42828   p29bae                       lol you had me at \"unpaid\"..\n",
       "131394  skjjvm  Do you think fine-tuning transformers in class...\n",
       "25050   o468ms  This is an excellent resource for reviewing ML...\n",
       "194142  rvya1w  You give me the numbers and I'll tell you the ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the content of our Comments Data\n",
    "comments_df = pd.read_csv('/home/studio-lab-user/sagemaker-studiolab-notebooks/Custom ChatGPT/data/Top_Posts_Comments.csv')\n",
    "comments_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4237b9-af54-4788-a8ee-3d705831b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Posts Data - (2987, 9)\n",
      "Shape of Comments Data - (223174, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Posts Data - {}\".format(posts_df.shape))\n",
    "print(\"Shape of Comments Data - {}\".format(comments_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294eeaa-ac03-4d96-8fa2-49b4f6ff2d1f",
   "metadata": {},
   "source": [
    "We have successfully extracted ~223K comments from top 1000 posts from popular sub-reddits related to Data Science.\n",
    "\n",
    "This data will be used to further create training data for our large language model and analytical purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130e030-bdd0-41e3-8001-13a51d01c89c",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "We have successfully extracted the useful Reddit data, and now we'll leverage the power of [In-Context Learning](http://ai.stanford.edu/blog/understanding-incontext/) ability of large language models.\n",
    "\n",
    "Concisely, In-context learning is a type of machine learning where the model is trained on a large corpus of text data, such as the GPT-3.5-Turbo model, which has been trained on a massive amount of text data to generate human-like responses to text prompts. In order to apply in-context learning to our Reddit data, we need to provide a large amount of relevant text data that the model can use to learn from. However, limited token size (Max Tokens ~ 4,096) can make it difficult to fit large text data into the model. To overcome this limitation, we used LLAMA-index to create text embeddings of our Reddit data.\n",
    "[LLAMA-index](https://gpt-index.readthedocs.io/en/latest/index.html) is a text embedding tool that creates compact representations of text data that can be used by machine learning models. These text embeddings can be used as inputs to our GPT-3.5-Turbo model, to improve its performance on specific tasks without running into token size limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e764142-b9e7-497c-94e1-330cd0d0a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important libraries used in\n",
    "# generating text embeddings\n",
    "\n",
    "from llama_index import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed924b2b-b9db-4616-8027-9a557fc623a5",
   "metadata": {},
   "source": [
    "#### Comments Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5040dc-b3ae-4d4f-bb30-6d939637aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining Comments with their respective Post ID\n",
    "comments_posts_merged = posts_df.merge(comments_df, on = 'post_id', how = 'left')\n",
    "\n",
    "# Deleting rows that doesn't contain any Comment\n",
    "comments_posts_merged = comments_posts_merged[~comments_posts_merged['comment'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a45efb-027b-46bd-b5fc-f72d3f2c3ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_title</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>I'm a Senior Data Scientist at Disney and I'm ...</td>\n",
       "      <td>Networking</td>\n",
       "      <td>There seems to be issues with the link in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>[D] Antipatterns in open sourced ML research code</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>[deleted]. Hah, initially I thought I would on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>It seems a lot of people want to get into the ...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>I understand people who want to change fields ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>PyTorch for Beginners - Building Neural Networks</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>Great for beginners who don't know where to st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>Imposter Syndrome is a problem for me and I th...</td>\n",
       "      <td>nan</td>\n",
       "      <td>I once attended an interview and they not only...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             post_title  flair_text  \\\n",
       "1029  I'm a Senior Data Scientist at Disney and I'm ...  Networking   \n",
       "2033  [D] Antipatterns in open sourced ML research code  Discussion   \n",
       "1124  It seems a lot of people want to get into the ...  Discussion   \n",
       "1436   PyTorch for Beginners - Building Neural Networks    Tutorial   \n",
       "1068  Imposter Syndrome is a problem for me and I th...         nan   \n",
       "\n",
       "                                                comment  \n",
       "1029  There seems to be issues with the link in the ...  \n",
       "2033  [deleted]. Hah, initially I thought I would on...  \n",
       "1124  I understand people who want to change fields ...  \n",
       "1436  Great for beginners who don't know where to st...  \n",
       "1068  I once attended an interview and they not only...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining all relevant textual information\n",
    "agg_comments_temp = comments_posts_merged[['post_title','flair_text', 'comment']].astype(str)\n",
    "agg_comments = agg_comments_temp.groupby(['post_title','flair_text'])['comment'].apply('. '.join).reset_index()\n",
    "agg_comments.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfaa4203-2645-4b34-9adf-86d288fc08b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Artificial Imagination\" - AI generated. My project. Why does everything look familiar but nothing is identifiable. The more you look, the less it makes sense. [deleted]. [deleted]. [deleted]. It's impressive how artificial intelligences are able to make more elaborate and less abstract representations over time. They're evolving in the right direction.. I am currently reading the book \"When Brains Dream\".  The current theory is that we dream to process the days events, to figure out the meaning and significance of that new information. \"Dreams are almost never an accurate replay of daytime events\". So similar to AI, our dreams are somehow looking for patterns in our experience to encode int\n"
     ]
    }
   ],
   "source": [
    "agg_comments['combined_text'] = agg_comments.astype(str).agg('. '.join, axis = 1)\n",
    "text_data = ' '.join(agg_comments['combined_text'])\n",
    "print(text_data[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9da383d5-6b72-4dc8-998d-f7dfde39b2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of Characters in aggregated texual data - 59605154\n"
     ]
    }
   ],
   "source": [
    "print('Total No. of Characters in aggregated textual data - {}'.format(len(text_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd280d-0d10-46d9-9f76-88f0ec57c7be",
   "metadata": {},
   "source": [
    "Now, we have aggregated all the relevant textual information from the comments we have extracted into a single text that can be used for fine-tuning/training our Large Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e2708b5-599f-42b8-be54-24f7c2569490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving text data in .txt format\n",
    "f = open('/home/studio-lab-user/sagemaker-studiolab-notebooks/Custom ChatGPT/data/train_data/train_data.txt', 'w')\n",
    "f.write(text_data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31bb1c-5407-4dd3-acfa-3c70beb4c487",
   "metadata": {},
   "source": [
    "#### Generating Text Embeddings/Indexes\n",
    "\n",
    "We will use the [Facebook AI Similarity Search (Faiss)](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) library for efficient similarity search between the input prompt and the existing corpus of data we have collected from Reddit API. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.\n",
    "We will then create the text embeddings which we will further use for quering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cbe2bf3-9cf9-4dd9-93df-066e878f8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing index generator function\n",
    "from src.index_generator import construct_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed6b086f-4c54-4de3-8d58-fd03a5498f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing OpenAI Credential info in local variables\n",
    "openai_key = config.get('OpenAI', 'secret_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0dcbc3a-e8a9-4100-bf78-3e3c003e6ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mText Embeddings created Successfully ! \n",
      "Stored in 'faiss_index' directory\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Constructing our indexes (ONLY NEED TO RUN ONCE! BE CAREFUL THAT THIS COSTS MONEY)\n",
    "training_data = 'data/train_data/train_data.txt'\n",
    "construct_index(training_data, openai_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994d90e-4ec8-4edf-9df3-0df5007d9f87",
   "metadata": {},
   "source": [
    "We have saved the indexes/embeddings of all the comments (textual data) we have collected from Reddit which we will further use to create a ChatBot based on that data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cpu-310:Python",
   "language": "python",
   "name": "conda-env-tf-cpu-310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
